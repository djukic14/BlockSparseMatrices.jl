var documenterSearchIndex = {"docs":
[{"location":"apiref/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"apiref/#BlockSparseMatrices.AbstractBlockMatrix","page":"API Reference","title":"BlockSparseMatrices.AbstractBlockMatrix","text":"abstract type AbstractBlockMatrix{T} <: LinearMap{T}\n\nAbstract type representing a block matrix with element type T. The block matrix is composed of a small number of smaller matrix blocks, allowing for efficient storage and computation.\n\nNotes\n\nThe AbstractBlockMatrix type serves as a base for concrete block matrix implementations, providing a common interface for linear algebra operations.\n\n\n\n\n\n","category":"type"},{"location":"apiref/#BlockSparseMatrices.BlockSparseMatrix","page":"API Reference","title":"BlockSparseMatrices.BlockSparseMatrix","text":"struct BlockSparseMatrix{T,M,P,S} <: AbstractBlockMatrix{T}\n\nA concrete implementation of a block sparse matrix, which is a sparse matrix composed of smaller dense matrix blocks.\n\nType Parameters\n\nT: The element type of the matrix.\nM: The type of the matrix blocks.\nP: The integer type used for indexing.\nS: The type of the scheduler.\n\nFields\n\nblocks: A vector of matrix blocks that comprise the block sparse matrix.\nrowindices: A vector where each element is a vector of row indices for the corresponding block.\ncolindices: A vector where each element is a vector of column indices for the corresponding block.\nsize: A tuple representing the size of the block sparse matrix.\ncolors: A vector of colors, where each color is a vector of block indices that can be processed in parallel without race conditions.\ntransposecolors: A vector of colors for the transpose matrix, where each color is a vector of block indices that can be processed in parallel without race conditions.\nscheduler: A scheduler that manages the parallel computation of matrix-vector products.\n\n\n\n\n\n","category":"type"},{"location":"apiref/#BlockSparseMatrices.BlockSparseMatrix-Tuple{Any, Any, Any, Tuple{Int64, Int64}}","page":"API Reference","title":"BlockSparseMatrices.BlockSparseMatrix","text":"BlockSparseMatrix(\n    blocks,\n    rowindices,\n    colindices,\n    size::Tuple{Int,Int};\n    coloringalgorithm=coloringalgorithm,\n    scheduler=SerialScheduler(),\n)\n\nConstructs a new BlockSparseMatrix instance from the given blocks, their indices, and size.\n\nArguments\n\nblocks: A vector of matrices representing the blocks.\nrowindices: A vector where each element is a vector of row indices for the corresponding block.\ncolindices: A vector where each element is a vector of column indices for the corresponding block.\nsize: A tuple representing the size of the block sparse matrix.\ncoloringalgorithm: The algorithm from GraphsColoring.jl used to color the blocks for parallel computation. Defaults to coloringalgorithm.\nscheduler: The scheduler used to manage parallel computation. Defaults to SerialScheduler().\n\nReturns\n\nA new BlockSparseMatrix instance constructed from the given blocks and size.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.ColorInfo","page":"API Reference","title":"BlockSparseMatrices.ColorInfo","text":"struct ColorInfo{R}\n\nA wrapper struct that holds conflict indices information used for graph coloring.\n\nType Parameters\n\nR: The type of the conflict indices container.\n\nFields\n\nconflictindices: A collection of indices that represent potential conflicts between blocks. Used to determine which blocks can be processed in parallel without race conditions.\n\n\n\n\n\n","category":"type"},{"location":"apiref/#BlockSparseMatrices.SymmetricBlockMatrix","page":"API Reference","title":"BlockSparseMatrices.SymmetricBlockMatrix","text":"struct SymmetricBlockMatrix{T,D,P,M,S} <: AbstractBlockMatrix{T}\n\nA concrete implementation of a symmetric block matrix, which is a block matrix where the off-diagonal blocks are shared between the upper and lower triangular parts. The diagonal blocks are symmetric as well.\n\nType Parameters\n\nT: The element type of the matrix.\nD: The type of the diagonal matrix blocks.\nP: The integer type used for indexing.\nM: The type of the off-diagonal matrix blocks.\nS: The type of the scheduler.\n\nFields\n\ndiagonals: A vector of diagonal matrix blocks.\ndiagonalindices: A vector where each element is a vector of indices for the corresponding diagonal block.\noffdiagonals: A vector of off-diagonal matrix blocks.\nrowindices: A vector where each element is a vector of row indices for the corresponding off-diagonal block.\ncolindices: A vector where each element is a vector of column indices for the corresponding off-diagonal block.\nsize: A tuple representing the size of the symmetric block matrix.\ndiagonalcolors: A vector of colors for the diagonal blocks, where each color is a vector of block indices that can be processed in parallel without race conditions.\noffdiagonalcolors: A vector of colors for the off-diagonal blocks, where each color is a vector of block indices that can be processed in parallel without race conditions.\ntransposeoffdiagonalcolors: A vector of colors for the transposed off-diagonal blocks, where each color is a vector of block indices that can be processed in parallel without race conditions.\nscheduler: A scheduler that manages the parallel computation of matrix-vector products.\n\n\n\n\n\n","category":"type"},{"location":"apiref/#BlockSparseMatrices.SymmetricBlockMatrix-Tuple{Any, Any, Any, Any, Any, Tuple{Int64, Int64}}","page":"API Reference","title":"BlockSparseMatrices.SymmetricBlockMatrix","text":"SymmetricBlockMatrix(\n    diagonals,\n    diagonalindices,\n    offdiagonals,\n    rowindices,\n    colindices,\n    size::Tuple{Int,Int};\n    scheduler=DynamicScheduler(),\n)\n\nConstructs a new SymmetricBlockMatrix instance from the given diagonal and off-diagonal blocks with their indices.\n\nArguments\n\ndiagonals: A vector of diagonal matrix blocks.\ndiagonalindices: A vector where each element is a vector of indices for the corresponding diagonal block.\noffdiagonals: A vector of off-diagonal matrix blocks.\nrowindices: A vector where each element is a vector of row indices for the corresponding off-diagonal block.\ncolindices: A vector where each element is a vector of column indices for the corresponding off-diagonal block.\nsize: A tuple representing the size of the symmetric block matrix.\nscheduler: The scheduler used to manage parallel computation. Defaults to DynamicScheduler().\n\nReturns\n\nA new SymmetricBlockMatrix instance constructed from the given blocks and indices.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.VariableBlockCompressedRowStorage","page":"API Reference","title":"BlockSparseMatrices.VariableBlockCompressedRowStorage","text":"struct VariableBlockCompressedRowStorage{T,M,P,S} <: AbstractBlockMatrix{T}\n\nA compressed row storage format for block sparse matrices with variable-sized blocks. This format stores blocks in row-major order, enabling efficient matrix-vector products and parallel computation.\n\nType Parameters\n\nT: The element type of the matrix.\nM: The type of the matrix blocks.\nP: The integer type used for indexing.\nS: The type of the scheduler.\n\nFields\n\nblocks: A vector of matrix blocks stored in row-major order.\nrowptr: A vector of pointers indicating the start of each block row in the blocks vector. Similar to CSR format, rowptr[i] points to the first block of the i-th block row.\ncolindices: A vector where each element is the starting column index of the corresponding block. The indices for each block must be contiguous (e.g., if a block starts at column 5 and has 3 columns, it occupies columns 5, 6, and 7).\nrowindices: A vector where each element is the starting row index of the corresponding block. The indices for each block must be contiguous (e.g., if a block starts at row 10 and has 4 rows, it occupies rows 10, 11, 12, and 13).\nsize: A tuple representing the total size of the matrix.\nscheduler: A scheduler that manages the parallel computation of matrix-vector products.\n\nNotes\n\nBlocks are sorted by row index first, then by column index within each row.\nThe compressed row storage format allows efficient parallel computation across block rows.\nThis format is particularly efficient for matrix-vector products and can handle variable-sized blocks.\nEach block must occupy a contiguous range of row and column indices.\n\n\n\n\n\n","category":"type"},{"location":"apiref/#BlockSparseMatrices.VariableBlockCompressedRowStorage-NTuple{4, Any}","page":"API Reference","title":"BlockSparseMatrices.VariableBlockCompressedRowStorage","text":"VariableBlockCompressedRowStorage(\n    matrices,\n    rowindices,\n    colindices,\n    matrixsize::Tuple{Int,Int};\n    scheduler = SerialScheduler()\n)\n\nConstructs a VariableBlockCompressedRowStorage from vectors of matrices and their corresponding row and column indices. The blocks are automatically sorted by row index first, then by column index, and stored in compressed row storage format.\n\nArguments\n\nmatrices: A vector of matrices representing the blocks.\nrowindices: A vector where each element is the starting row index of the corresponding block. Each block must occupy contiguous row indices starting from this value.\ncolindices: A vector where each element is the starting column index of the corresponding block. Each block must occupy contiguous column indices starting from this value.\nmatrixsize: The total size of the matrix as a tuple (nrows, ncols).\nscheduler: A scheduler for parallel computation. Defaults to SerialScheduler().\n\nReturns\n\nA VariableBlockCompressedRowStorage instance with compressed row storage format.\n\nNotes\n\nBlocks do not need to be provided in sorted order; they will be sorted internally.\nThe row and column indices for each block must be contiguous.\nBlocks with the same row index are grouped together in the compressed format.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.VariableBlockCompressedRowStorage-Union{Tuple{BlockSparseMatrix{T, M}}, Tuple{M}, Tuple{T}} where {T, M}","page":"API Reference","title":"BlockSparseMatrices.VariableBlockCompressedRowStorage","text":"VariableBlockCompressedRowStorage(\n    bsm::BlockSparseMatrix;\n    scheduler=bsm.scheduler\n)\n\nConverts a BlockSparseMatrix to VariableBlockCompressedRowStorage format.\n\nArguments\n\nbsm: A BlockSparseMatrix to convert.\nscheduler: A scheduler for parallel computation. Defaults to the scheduler from bsm.\n\nReturns\n\nA VariableBlockCompressedRowStorage instance in compressed row storage format.\n\nNotes\n\nNo sanity checks are performed on the input matrix. It is assumed that the blocks in the BlockSparseMatrix have contiguous row and column indices.\nThe conversion uses lazy functors to avoid unnecessary memory allocations during construction.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.VariableBlockCompressedRowStorage-Union{Tuple{SymmetricBlockMatrix{T, D, P, M}}, Tuple{M}, Tuple{P}, Tuple{D}, Tuple{T}} where {T, D, P, M}","page":"API Reference","title":"BlockSparseMatrices.VariableBlockCompressedRowStorage","text":"VariableBlockCompressedRowStorage(\n    sbm::SymmetricBlockMatrix;\n    scheduler=sbm.scheduler\n)\n\nConverts a SymmetricBlockMatrix to VariableBlockCompressedRowStorage format. The symmetric structure is expanded by including both the diagonal blocks, off-diagonal blocks, and their transposes explicitly.\n\nArguments\n\nsbm: A SymmetricBlockMatrix to convert.\nscheduler: A scheduler for parallel computation. Defaults to the scheduler from sbm.\n\nReturns\n\nA VariableBlockCompressedRowStorage instance in compressed row storage format.\n\nNotes\n\nNo sanity checks are performed on the input matrix. It is assumed that the blocks in the SymmetricBlockMatrix have contiguous row and column indices.\nThe conversion expands the symmetric structure: diagonal blocks are included once, while off-diagonal blocks are included twice (once as-is and once transposed).\nThe conversion uses lazy functors to avoid unnecessary memory allocations during construction.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.block-Tuple{BlockSparseMatrix, Any}","page":"API Reference","title":"BlockSparseMatrices.block","text":"block(A::BlockSparseMatrix, i)\n\nReturns the i-th block of the given BlockSparseMatrix instance.\n\nArguments\n\nA: The BlockSparseMatrix instance to query.\ni: The index of the block to retrieve.\n\nReturns\n\nThe i-th block of the BlockSparseMatrix.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.colors-Tuple{BlockSparseMatrix}","page":"API Reference","title":"BlockSparseMatrices.colors","text":"colors(A::BlockSparseMatrix)\n\nReturns the colors used for multithreading in matrix-vector for the given BlockSparseMatrix. These colors are created using GraphsColoring.jl and represent a partitioning of the blocks into sets that can be processed in parallel without race conditions.\n\nArguments\n\nA: The BlockSparseMatrix instance to query.\n\nReturns\n\nA collection of colors, where each color is a vector of block indices that can be processed in parallel.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.diagonal-Tuple{SymmetricBlockMatrix, Any}","page":"API Reference","title":"BlockSparseMatrices.diagonal","text":"diagonal(A::SymmetricBlockMatrix, i)\n\nReturns the i-th diagonal block of the given SymmetricBlockMatrix instance.\n\nArguments\n\nA: The SymmetricBlockMatrix instance to query.\ni: The index of the diagonal block to retrieve.\n\nReturns\n\nThe i-th diagonal block of the SymmetricBlockMatrix.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.diagonalcolors-Tuple{SymmetricBlockMatrix}","page":"API Reference","title":"BlockSparseMatrices.diagonalcolors","text":"diagonalcolors(A::SymmetricBlockMatrix)\n\nReturns the colors used for the diagonal blocks of the given SymmetricBlockMatrix instance. These colors are used to coordinate parallel computation and avoid race conditions.\n\nArguments\n\nA: The SymmetricBlockMatrix instance to query.\n\nReturns\n\nA vector of colors, where each color is a vector of diagonal block indices that can be processed in parallel without race conditions.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.eachblockindex-Tuple{BlockSparseMatrix}","page":"API Reference","title":"BlockSparseMatrices.eachblockindex","text":"eachblockindex(A::BlockSparseMatrix)\n\nReturns an iterator over the indices of the blocks in the given BlockSparseMatrix instance.\n\nArguments\n\nA: The BlockSparseMatrix instance to query.\n\nReturns\n\nAn iterator that yields the indices of the blocks in the BlockSparseMatrix.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.eachdiagonalindex-Tuple{SymmetricBlockMatrix}","page":"API Reference","title":"BlockSparseMatrices.eachdiagonalindex","text":"eachdiagonalindex(A::SymmetricBlockMatrix)\n\nReturns an iterator over the indices of the diagonal blocks in the given SymmetricBlockMatrix instance.\n\nArguments\n\nA: The SymmetricBlockMatrix instance to query.\n\nReturns\n\nAn iterator that yields the indices of the diagonal blocks in the SymmetricBlockMatrix.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.eachoffdiagonalindex-Tuple{SymmetricBlockMatrix}","page":"API Reference","title":"BlockSparseMatrices.eachoffdiagonalindex","text":"eachoffdiagonalindex(A::SymmetricBlockMatrix)\n\nReturns an iterator over the indices of the off-diagonal blocks in the given SymmetricBlockMatrix instance.\n\nArguments\n\nA: The SymmetricBlockMatrix instance to query.\n\nReturns\n\nAn iterator that yields the indices of the off-diagonal blocks in the SymmetricBlockMatrix.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.offdiagonal-Tuple{SymmetricBlockMatrix, Any}","page":"API Reference","title":"BlockSparseMatrices.offdiagonal","text":"offdiagonal(A::SymmetricBlockMatrix, i)\n\nReturns the i-th off-diagonal block of the given SymmetricBlockMatrix instance.\n\nArguments\n\nA: The SymmetricBlockMatrix instance to query.\ni: The index of the off-diagonal block to retrieve.\n\nReturns\n\nThe i-th off-diagonal block of the SymmetricBlockMatrix.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.offdiagonalcolors-Tuple{SymmetricBlockMatrix}","page":"API Reference","title":"BlockSparseMatrices.offdiagonalcolors","text":"offdiagonalcolors(A::SymmetricBlockMatrix)\n\nReturns the colors used for the off-diagonal blocks of the given SymmetricBlockMatrix instance. These colors are used to coordinate parallel computation and avoid race conditions.\n\nArguments\n\nA: The SymmetricBlockMatrix instance to query.\n\nReturns\n\nA vector of colors, where each color is a vector of off-diagonal block indices that can be processed in parallel without race conditions.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.rowcolvals-Union{Tuple{M}, Tuple{Z}} where {Z<:BlockSparseMatrix, M<:Union{LinearMaps.AdjointMap{<:Any, Z}, LinearMaps.TransposeMap{<:Any, Z}, Z}}","page":"API Reference","title":"BlockSparseMatrices.rowcolvals","text":"rowcolvals(A)\n\nExtracts the row, column, and value indices from a block-sparse matrix A such that a sparse matrix can be constructed.\n\nArguments\n\nA: A block-sparse matrix.\n\nReturns\n\nrows: An array of row indices.\ncols: An array of column indices.\nvals: An array of values.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.scheduler-Tuple{BlockSparseMatrices.AbstractBlockMatrix}","page":"API Reference","title":"BlockSparseMatrices.scheduler","text":"scheduler(A::AbstractBlockMatrix)\n\nReturns the scheduler associated with the given AbstractBlockMatrix instance. This scheduler is responsible for managing the parallel computation of matrix-vector products.\n\nReturns\n\nThe scheduler associated with the matrix.\n\nNotes\n\nThe scheduler is used to coordinate the computation of matrix-vector product.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.transposecolors-Tuple{BlockSparseMatrix}","page":"API Reference","title":"BlockSparseMatrices.transposecolors","text":"transposecolors(A::BlockSparseMatrix)\n\nReturns the colors used for multithreading in the transposed matrix-vector product computations for the given BlockSparseMatrix. These colors are created using GraphsColoring.jl and represent a partitioning of the blocks into sets that can be processed in parallel without race conditions.\n\nArguments\n\nA: The BlockSparseMatrix instance to query.\n\nReturns\n\nA collection of colors, where each color is a vector of block indices that can be processed in parallel.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#BlockSparseMatrices.transposeoffdiagonalcolors-Tuple{SymmetricBlockMatrix}","page":"API Reference","title":"BlockSparseMatrices.transposeoffdiagonalcolors","text":"transposeoffdiagonalcolors(A::SymmetricBlockMatrix)\n\nReturns the colors used for the transposed off-diagonal blocks of the given SymmetricBlockMatrix instance. These colors are used to coordinate parallel computation and avoid race conditions when computing the transpose of the matrix.\n\nArguments\n\nA: The SymmetricBlockMatrix instance to query.\n\nReturns\n\nA vector of colors, where each color is a vector of transposed off-diagonal block indices that can be processed in parallel without race conditions.\n\n\n\n\n\n","category":"method"},{"location":"apiref/#GraphsColoring.conflicts-Tuple{BlockSparseMatrices.ColorInfo}","page":"API Reference","title":"GraphsColoring.conflicts","text":"conflicts(blocks::ColorInfo)\n\nComputes the conflicts between blocks for the purpose of graph coloring using GraphsColoring.jl. This function is used to determine the coloring of blocks for multithreading in the matrix-vector product, ensuring that blocks with no conflicts can be processed in parallel.\n\nArguments\n\nblocks: A ColorInfo object containing the conflict indices information.\n\nReturns\n\nA tuple containing:\nAn iterator over block indices\nA ConflictFunctor wrapping the computed conflict indices\nA range representing the maximum conflict index\n\nNotes\n\nBlocks with no conflicts (i.e., blocks that do not overlap in their conflict indices) can be processed in parallel.\nThe colors are used to group blocks into sets that can be processed in parallel, avoiding race conditions and ensuring efficient multithreading in the matrix-vector product.\n\n\n\n\n\n","category":"method"},{"location":"vbcrs/#VariableBlockCompressedRowStorage","page":"VariableBlockCompressedRowStorage","title":"VariableBlockCompressedRowStorage","text":"VariableBlockCompressedRowStorage is a memory‑efficient storage format for block‑sparse matrices where blocks are aligned along contiguous row and column ranges.   This format is particularly useful when the matrix structure naturally groups into rectangular blocks with varying sizes.\n\nnote: Note\nThe indices must be contiguous ranges for VBCRS, and no sanity check is performed during construction.\n\nBelow you will find a compact, step‑by‑step guide that shows how to\n\nBuild a VBCRS matrix from a list of blocks and their starting indices.  \nMultiply it with vectors (including transposes).  \nConvert it from a BlockSparseMatrix.\nConvert it to a SparseMatrixCSC.\n\n","category":"section"},{"location":"vbcrs/#Constructing-a-VariableBlockCompressedRowStorage","page":"VariableBlockCompressedRowStorage","title":"Constructing a VariableBlockCompressedRowStorage","text":"The key difference from BlockSparseMatrix is that VBCRS only needs the first index of each contiguous range rather than the full list of indices.\n\nusing CompScienceMeshes, BEAST, H2Trees #hide\nusing UnicodePlots #hide\nusing BlockSparseMatrices #hide\n\nfunction sortbasis!(tree, space) #hide\n    newindices = _sortbasisindices!(tree, space) #hide\n    space.fns .= space.fns[newindices] #hide\n    space.pos .= space.pos[newindices] #hide\n    return space #hide\nend #hide\n\nfunction _sortbasisindices!(tree, basis) #hide\n    newindices = Vector{Int}(undef, numfunctions(basis)) #hide\n\n    lastindex = 0 #hide\n    for node in H2Trees.leaves(tree) #hide\n        points = H2Trees.values(tree, node) #hide\n\n        newpositionids = (1:length(points)) .+ lastindex #hide\n\n        newindices[points] .= newpositionids #hide\n\n        tree(node).data.values .= newpositionids #hide\n\n        lastindex += length(points) #hide\n    end #hide\n\n    return newindices #hide\nend #hide\n\nm = meshcuboid(1.0, 1.0, 1.0, 0.04) #hide\nX = raviartthomas(m) #hide\ntree = TwoNTree(X, 0.05) #hide\nX = sortbasis!(tree, X) #hide\n\nfor leaf in H2Trees.leaves(tree) #hide\n    vals = H2Trees.values(tree, leaf) #hide\n    @assert vals == vals[begin]:vals[end] #hide\nend #hide\n\ntestindices = Vector{Int}[] #hide\ntrialindices = Vector{Int}[] #hide\n\nfor node in H2Trees.leaves(tree) #hide\n    for nearnode in H2Trees.NearNodeIterator(tree, node) #hide\n        push!(testindices, collect(H2Trees.values(tree, node))) #hide\n        push!(trialindices, collect(H2Trees.values(tree, nearnode))) #hide\n    end #hide\nend #hide\n\nfor i in eachindex(testindices) #hide\n    sort!(testindices[i]) #hide\n    sort!(trialindices[i]) #hide\n    @assert testindices[i] == testindices[i][begin]:testindices[i][end] #hide\n    @assert trialindices[i] == trialindices[i][begin]:trialindices[i][end] #hide\nend #hide\n\nblocks = Matrix{ComplexF64}[] #hide\nfor i in eachindex(testindices) #hide\n    push!(blocks, randn(ComplexF64, length(testindices[i]), length(trialindices[i]))) #hide\nend #hide\nsizematrix = (numfunctions(X), numfunctions(X)) #hide\n\nB = BlockSparseMatrix(blocks, testindices, trialindices, sizematrix)\n\nHere, testindices[1] and trialindices[1] show contiguous ranges for the first block\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\nfunction sortbasis!(tree, space) #hide\n    newindices = _sortbasisindices!(tree, space) #hide\n    space.fns .= space.fns[newindices] #hide\n    space.pos .= space.pos[newindices] #hide\n    return space #hide\nend #hide\n\nfunction _sortbasisindices!(tree, basis) #hide\n    newindices = Vector{Int}(undef, numfunctions(basis)) #hide\n\n    lastindex = 0 #hide\n    for node in H2Trees.leaves(tree) #hide\n        points = H2Trees.values(tree, node) #hide\n\n        newpositionids = (1:length(points)) .+ lastindex #hide\n\n        newindices[points] .= newpositionids #hide\n\n        tree(node).data.values .= newpositionids #hide\n\n        lastindex += length(points) #hide\n    end #hide\n\n    return newindices #hide\nend #hide\n\nm = meshcuboid(1.0, 1.0, 1.0, 0.04) #hide\nX = raviartthomas(m) #hide\ntree = TwoNTree(X, 0.05) #hide\nX = sortbasis!(tree, X) #hide\n\nfor leaf in H2Trees.leaves(tree) #hide\n    vals = H2Trees.values(tree, leaf) #hide\n    @assert vals == vals[begin]:vals[end] #hide\nend #hide\n\ntestindices = Vector{Int}[] #hide\ntrialindices = Vector{Int}[] #hide\n\nfor node in H2Trees.leaves(tree) #hide\n    for nearnode in H2Trees.NearNodeIterator(tree, node) #hide\n        push!(testindices, collect(H2Trees.values(tree, node))) #hide\n        push!(trialindices, collect(H2Trees.values(tree, nearnode))) #hide\n    end #hide\nend #hide\n\nfor i in eachindex(testindices) #hide\n    sort!(testindices[i]) #hide\n    sort!(trialindices[i]) #hide\n    @assert testindices[i] == testindices[i][begin]:testindices[i][end] #hide\n    @assert trialindices[i] == trialindices[i][begin]:trialindices[i][end] #hide\nend #hide\n\nblocks = Matrix{ComplexF64}[] #hide\nfor i in eachindex(testindices) #hide\n    push!(blocks, randn(ComplexF64, length(testindices[i]), length(trialindices[i]))) #hide\nend #hide\nsizematrix = (numfunctions(X), numfunctions(X)) #hide\n\ntestindices[1], trialindices[1] \n\nNow we can construct the VBCRS matrix by passing only the first index of each contiguous range\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\nfunction sortbasis!(tree, space) #hide\n    newindices = _sortbasisindices!(tree, space) #hide\n    space.fns .= space.fns[newindices] #hide\n    space.pos .= space.pos[newindices] #hide\n    return space #hide\nend #hide\n\nfunction _sortbasisindices!(tree, basis) #hide\n    newindices = Vector{Int}(undef, numfunctions(basis)) #hide\n\n    lastindex = 0 #hide\n    for node in H2Trees.leaves(tree) #hide\n        points = H2Trees.values(tree, node) #hide\n\n        newpositionids = (1:length(points)) .+ lastindex #hide\n\n        newindices[points] .= newpositionids #hide\n\n        tree(node).data.values .= newpositionids #hide\n\n        lastindex += length(points) #hide\n    end #hide\n\n    return newindices #hide\nend #hide\n\nm = meshcuboid(1.0, 1.0, 1.0, 0.04) #hide\nX = raviartthomas(m) #hide\ntree = TwoNTree(X, 0.05) #hide\nX = sortbasis!(tree, X) #hide\n\nfor leaf in H2Trees.leaves(tree) #hide\n    vals = H2Trees.values(tree, leaf) #hide\n    @assert vals == vals[begin]:vals[end] #hide\nend #hide\n\ntestindices = Vector{Int}[] #hide\ntrialindices = Vector{Int}[] #hide\n\nfor node in H2Trees.leaves(tree) #hide\n    for nearnode in H2Trees.NearNodeIterator(tree, node) #hide\n        push!(testindices, collect(H2Trees.values(tree, node))) #hide\n        push!(trialindices, collect(H2Trees.values(tree, nearnode))) #hide\n    end #hide\nend #hide\n\nfor i in eachindex(testindices) #hide\n    sort!(testindices[i]) #hide\n    sort!(trialindices[i]) #hide\n    @assert testindices[i] == testindices[i][begin]:testindices[i][end] #hide\n    @assert trialindices[i] == trialindices[i][begin]:trialindices[i][end] #hide\nend #hide\n\nblocks = Matrix{ComplexF64}[] #hide\nfor i in eachindex(testindices) #hide\n    push!(blocks, randn(ComplexF64, length(testindices[i]), length(trialindices[i]))) #hide\nend #hide\nsizematrix = (numfunctions(X), numfunctions(X)) #hide\n\nV = VariableBlockCompressedRowStorage(\n    blocks, first.(testindices), first.(trialindices), sizematrix\n)\n\nV now behaves like a regular matrix but with significantly reduced storage overhead compared to storing full index arrays.","category":"section"},{"location":"vbcrs/#Matrix–Vector-Products","page":"VariableBlockCompressedRowStorage","title":"Matrix–Vector Products","text":"using CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\nfunction sortbasis!(tree, space) #hide\n    newindices = _sortbasisindices!(tree, space) #hide\n    space.fns .= space.fns[newindices] #hide\n    space.pos .= space.pos[newindices] #hide\n    return space #hide\nend #hide\n\nfunction _sortbasisindices!(tree, basis) #hide\n    newindices = Vector{Int}(undef, numfunctions(basis)) #hide\n\n    lastindex = 0 #hide\n    for node in H2Trees.leaves(tree) #hide\n        points = H2Trees.values(tree, node) #hide\n\n        newpositionids = (1:length(points)) .+ lastindex #hide\n\n        newindices[points] .= newpositionids #hide\n\n        tree(node).data.values .= newpositionids #hide\n\n        lastindex += length(points) #hide\n    end #hide\n\n    return newindices #hide\nend #hide\n\nm = meshcuboid(1.0, 1.0, 1.0, 0.04) #hide\nX = raviartthomas(m) #hide\ntree = TwoNTree(X, 0.05) #hide\nX = sortbasis!(tree, X) #hide\n\nfor leaf in H2Trees.leaves(tree) #hide\n    vals = H2Trees.values(tree, leaf) #hide\n    @assert vals == vals[begin]:vals[end] #hide\nend #hide\n\ntestindices = Vector{Int}[] #hide\ntrialindices = Vector{Int}[] #hide\n\nfor node in H2Trees.leaves(tree) #hide\n    for nearnode in H2Trees.NearNodeIterator(tree, node) #hide\n        push!(testindices, collect(H2Trees.values(tree, node))) #hide\n        push!(trialindices, collect(H2Trees.values(tree, nearnode))) #hide\n    end #hide\nend #hide\n\nfor i in eachindex(testindices) #hide\n    sort!(testindices[i]) #hide\n    sort!(trialindices[i]) #hide\n    @assert testindices[i] == testindices[i][begin]:testindices[i][end] #hide\n    @assert trialindices[i] == trialindices[i][begin]:trialindices[i][end] #hide\nend #hide\n\nblocks = Matrix{ComplexF64}[] #hide\nfor i in eachindex(testindices) #hide\n    push!(blocks, randn(ComplexF64, length(testindices[i]), length(trialindices[i]))) #hide\nend #hide\nsizematrix = (numfunctions(X), numfunctions(X)) #hide\n\nB = BlockSparseMatrix(blocks, testindices, trialindices, sizematrix) #hide\n\nV = VariableBlockCompressedRowStorage( #hide\n    blocks, first.(testindices), first.(trialindices), sizematrix #hide\n) #hide\n\nx = randn(ComplexF64, size(V,2)) #hide\n\n@time V * x\n@time V' * x\n@time transpose(V) * x\nnothing # hide\n\nAll three operations are implemented in pure Julia and respect the block‑sparsity, with cache-friendly access patterns due to the compressed row storage layout.","category":"section"},{"location":"vbcrs/#Converting-Between-Formats","page":"VariableBlockCompressedRowStorage","title":"Converting Between Formats","text":"You can directly convert a BlockSparseMatrix to VBCRS format when the index structure satisfies the contiguity requirement\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\nfunction sortbasis!(tree, space) #hide\n    newindices = _sortbasisindices!(tree, space) #hide\n    space.fns .= space.fns[newindices] #hide\n    space.pos .= space.pos[newindices] #hide\n    return space #hide\nend #hide\n\nfunction _sortbasisindices!(tree, basis) #hide\n    newindices = Vector{Int}(undef, numfunctions(basis)) #hide\n\n    lastindex = 0 #hide\n    for node in H2Trees.leaves(tree) #hide\n        points = H2Trees.values(tree, node) #hide\n\n        newpositionids = (1:length(points)) .+ lastindex #hide\n\n        newindices[points] .= newpositionids #hide\n\n        tree(node).data.values .= newpositionids #hide\n\n        lastindex += length(points) #hide\n    end #hide\n\n    return newindices #hide\nend #hide\n\nm = meshcuboid(1.0, 1.0, 1.0, 0.04) #hide\nX = raviartthomas(m) #hide\ntree = TwoNTree(X, 0.05) #hide\nX = sortbasis!(tree, X) #hide\n\nfor leaf in H2Trees.leaves(tree) #hide\n    vals = H2Trees.values(tree, leaf) #hide\n    @assert vals == vals[begin]:vals[end] #hide\nend #hide\n\ntestindices = Vector{Int}[] #hide\ntrialindices = Vector{Int}[] #hide\n\nfor node in H2Trees.leaves(tree) #hide\n    for nearnode in H2Trees.NearNodeIterator(tree, node) #hide\n        push!(testindices, collect(H2Trees.values(tree, node))) #hide\n        push!(trialindices, collect(H2Trees.values(tree, nearnode))) #hide\n    end #hide\nend #hide\n\nfor i in eachindex(testindices) #hide\n    sort!(testindices[i]) #hide\n    sort!(trialindices[i]) #hide\n    @assert testindices[i] == testindices[i][begin]:testindices[i][end] #hide\n    @assert trialindices[i] == trialindices[i][begin]:trialindices[i][end] #hide\nend #hide\n\nblocks = Matrix{ComplexF64}[] #hide\nfor i in eachindex(testindices) #hide\n    push!(blocks, randn(ComplexF64, length(testindices[i]), length(trialindices[i]))) #hide\nend #hide\nsizematrix = (numfunctions(X), numfunctions(X)) #hide\n\nB = BlockSparseMatrix(blocks, testindices, trialindices, sizematrix)\n\nV = VariableBlockCompressedRowStorage(B) \n\nnote: Note\nConversion also works with SymmetricBlockMatrix.\n\nSometimes you need a SparseMatrixCSC. The conversion is straightforward\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\nusing SparseArrays # hide\n\nfunction sortbasis!(tree, space) #hide\n    newindices = _sortbasisindices!(tree, space) #hide\n    space.fns .= space.fns[newindices] #hide\n    space.pos .= space.pos[newindices] #hide\n    return space #hide\nend #hide\n\nfunction _sortbasisindices!(tree, basis) #hide\n    newindices = Vector{Int}(undef, numfunctions(basis)) #hide\n\n    lastindex = 0 #hide\n    for node in H2Trees.leaves(tree) #hide\n        points = H2Trees.values(tree, node) #hide\n\n        newpositionids = (1:length(points)) .+ lastindex #hide\n\n        newindices[points] .= newpositionids #hide\n\n        tree(node).data.values .= newpositionids #hide\n\n        lastindex += length(points) #hide\n    end #hide\n\n    return newindices #hide\nend #hide\n\nm = meshcuboid(1.0, 1.0, 1.0, 0.04) #hide\nX = raviartthomas(m) #hide\ntree = TwoNTree(X, 0.05) #hide\nX = sortbasis!(tree, X) #hide\n\nfor leaf in H2Trees.leaves(tree) #hide\n    vals = H2Trees.values(tree, leaf) #hide\n    @assert vals == vals[begin]:vals[end] #hide\nend #hide\n\ntestindices = Vector{Int}[] #hide\ntrialindices = Vector{Int}[] #hide\n\nfor node in H2Trees.leaves(tree) #hide\n    for nearnode in H2Trees.NearNodeIterator(tree, node) #hide\n        push!(testindices, collect(H2Trees.values(tree, node))) #hide\n        push!(trialindices, collect(H2Trees.values(tree, nearnode))) #hide\n    end #hide\nend #hide\n\nfor i in eachindex(testindices) #hide\n    sort!(testindices[i]) #hide\n    sort!(trialindices[i]) #hide\n    @assert testindices[i] == testindices[i][begin]:testindices[i][end] #hide\n    @assert trialindices[i] == trialindices[i][begin]:trialindices[i][end] #hide\nend #hide\n\nblocks = Matrix{ComplexF64}[] #hide\nfor i in eachindex(testindices) #hide\n    push!(blocks, randn(ComplexF64, length(testindices[i]), length(trialindices[i]))) #hide\nend #hide\nsizematrix = (numfunctions(X), numfunctions(X)) #hide\n\nB = BlockSparseMatrix(blocks, testindices, trialindices, sizematrix) #hide\n\nV = VariableBlockCompressedRowStorage( \n    blocks, first.(testindices), first.(trialindices), sizematrix \n) \n\ns = sparse(V)\n\nThe VBCRS format is particularly memory‑efficient for this storage pattern, as it only stores the starting index of each contiguous block range rather than explicit lists of all indices.","category":"section"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"In order to contribute to this package directly create a pull request against the main branch. Before doing so please: \n\nFollow the style of the surrounding code.\nSupplement the documentation.\nWrite tests and check that no errors occur.\n\n","category":"section"},{"location":"contributing/#Style","page":"Contributing","title":"Style","text":"For a consistent style the JuliaFormatter.jl package is used which enforces the style defined in the .JuliaFormatter.toml file. To follow this style simply run\n\nusing JuliaFormatter\nformat(pkgdir(BlockSparseMatrices))\n\nnote: Note\nThat all files follow the JuliaFormatter style is tested during the unit tests. Hence, do not forget to execute the two lines above. Otherwise, the tests are likely to not pass.\n\n","category":"section"},{"location":"contributing/#Documentation","page":"Contributing","title":"Documentation","text":"Add documentation for any changes or new features following the style of the existing documentation. For more information you can have a look at the Documenter.jl documentation.\n\n","category":"section"},{"location":"contributing/#tests","page":"Contributing","title":"Tests","text":"Write tests for your code changes and verify that no errors occur, e.g., by running\n\nusing Pkg\nPkg.test(\"BlockSparseMatrices.jl\")\n\nFor a detailed information on which parts are tested the coverage can be evaluated on your local machine, e.g., by\n\nusing Pkg\nPkg.test(\"BlockSparseMatrices\"; coverage=true, julia_args=[\"-t 4\"])\n\n# determine coverage\nusing Coverage\nsrc_folder = pkgdir(BlockSparseMatrices) * \"/src\"\ncoverage   = process_folder(src_folder)\nLCOV.writefile(\"path-to-folder-you-like\" * \"BlockSparseMatrices.lcov.info\", coverage)\n\nclean_folder(src_folder) # delete .cov files\n\n# extract information about coverage\ncovered_lines, total_lines = get_summary(coverage)\n@info \"Current coverage:\\n$covered_lines of $total_lines lines ($(round(Int, covered_lines / total_lines * 100)) %)\"\n\nIn Visual Studio Code the Coverage Gutters plugin can be used to visualize the tested lines of the code by inserting the path of the BlockSparseMatrices.lcov.info file in the settings.","category":"section"},{"location":"block/#BlockSparseMatrix","page":"BlockSparseMatrix","title":"BlockSparseMatrix","text":"A BlockSparseMatrix is a matrices that is (at least) sparse at the block level.   This can, for example, occur when the matrix originates from a fast multipole method (FMM) or any other algorithm that naturally groups rows and columns into interacting blocks (e.g. near‑field interactions).\n\nBelow you will find a compact, step‑by‑step guide that shows how to\n\nBuild a block‑sparse matrix from a list of dense blocks and their index maps.  \nMultiply it with vectors (including transposes).  \nConvert it to a standard SparseMatrixCSC for comparison.  \nEnable multi‑threaded construction with OhMyThreads.\n\n","category":"section"},{"location":"block/#Constructing-a-BlockSparseMatrix","page":"BlockSparseMatrix","title":"Constructing a BlockSparseMatrix","text":"using CompScienceMeshes, BEAST, H2Trees\nusing UnicodePlots\nusing BlockSparseMatrices\n\nm = meshsphere(1.0, 0.1)\nX = raviartthomas(m)\ntree = TwoNTree(X, 0.2)\ncolvalues, rowvalues = H2Trees.nearinteractions(tree)\n\nblocks = Matrix{Float64}[]\nfor i in eachindex(colvalues)\n    push!(blocks, randn(Float64, length(colvalues[i]), length(rowvalues[i])))\nend\n\nB = BlockSparseMatrix(blocks, colvalues, rowvalues, (numfunctions(X), numfunctions(X)))\n\nB now behaves like a regular matrix: you can query its size, inspect its blocks, etc.","category":"section"},{"location":"block/#Matrix–Vector-Products","page":"BlockSparseMatrix","title":"Matrix–Vector Products","text":"using CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\nm = meshsphere(1.0, 0.1) # hide\nX = raviartthomas(m) # hide\ntree = TwoNTree(X, 0.2) # hide\ncolvalues, rowvalues = H2Trees.nearinteractions(tree) # hide\n\nblocks = Matrix{Float64}[] # hide\nfor i in eachindex(colvalues) # hide\n    push!(blocks, randn(Float64, length(colvalues[i]), length(rowvalues[i]))) # hide\nend # hide\n\nB = BlockSparseMatrix(blocks, colvalues, rowvalues, (numfunctions(X), numfunctions(X))) # hide\n\ny = randn(Float64, size(B, 1))\n@time B * y\n@time B' * y\n@time transpose(B) * y\nnothing # hide\n\nAll three operations are implemented in pure Julia and respect the block‑sparsity, so they are typically much faster than converting the matrix to a dense format first.","category":"section"},{"location":"block/#Converting-to-a-Classical-Sparse-Matrix","page":"BlockSparseMatrix","title":"Converting to a Classical Sparse Matrix","text":"Sometimes you need a SparseMatrixCSC. The conversion is straightforward. You can compare the memory footprints:\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\nm = meshsphere(1.0, 0.1) # hide\nX = raviartthomas(m) # hide\ntree = TwoNTree(X, 0.2) # hide\ncolvalues, rowvalues = H2Trees.nearinteractions(tree) # hide\n\nblocks = Matrix{Float64}[] # hide\nfor i in eachindex(colvalues) # hide\n    push!(blocks, randn(Float64, length(colvalues[i]), length(rowvalues[i]))) # hide\nend # hide\n\nB = BlockSparseMatrix(blocks, colvalues, rowvalues, (numfunctions(X), numfunctions(X))) # hide\n\nusing SparseArrays\nBsparse = sparse(B)\n\n@show Base.format_bytes(Base.summarysize(B));\n@show Base.format_bytes(Base.summarysize(Bsparse));\nnothing # hide\n\nIn specific examples BlockSparseMatrix consumes less memory.","category":"section"},{"location":"block/#Multi‑Threaded-Construction-(Optional)","page":"BlockSparseMatrix","title":"Multi‑Threaded Construction (Optional)","text":"Depending on the example, the block-coloring step can be a bottleneck, thus multithreading is switched-off by default. To enable multithreading you can determine a scheduler from OhMyThreads.\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\nm = meshsphere(1.0, 0.1) # hide\nX = raviartthomas(m) # hide\ntree = TwoNTree(X, 0.2) # hide\ncolvalues, rowvalues = H2Trees.nearinteractions(tree) # hide\n\nblocks = Matrix{Float64}[] # hide\nfor i in eachindex(colvalues) # hide\n    push!(blocks, randn(Float64, length(colvalues[i]), length(rowvalues[i]))) # hide\nend # hide\n\nusing OhMyThreads\nB = BlockSparseMatrix(\n    blocks,\n    colvalues,\n    rowvalues,\n    (numfunctions(X), numfunctions(X));\n    scheduler=DynamicScheduler(),\n)\n\nnothing # hide","category":"section"},{"location":"#BlockSparseMatrices.jl","page":"Introduction","title":"BlockSparseMatrices.jl","text":"BlockSparseMatrices.jl provides a representation for sparse matrices that are composed of a limited number of (dense) blocks.   It also includes specialized algorithms for symmetric block‑sparse matrices, storing only the necessary half of the off‑diagonal blocks.  ","category":"section"},{"location":"#Key-Features","page":"Introduction","title":"Key Features","text":"Block‑sparse storage – the matrix is built from a small set of (dense) sub‑blocks.  \nSymmetric support – for symmetric block‑sparse matrices only the lower (or upper) triangular block‑structure is kept, reducing memory usage.  \nMultithreaded matrix–vector multiplication – leverages  OhMyThreads and GraphsColoring for safe parallelism.  ","category":"section"},{"location":"#Implemented-Operations","page":"Introduction","title":"Implemented Operations","text":"Operation Description\n* (matrix‑vector product) Fast, multithreaded multiplication.\ntranspose / adjoint Returns the (adjoint) transpose of a block‑sparse matrix.\nVisualization Visual inspection via UnicodePlots.\nsparse Convert to a standard SparseMatrixCSC from SparseArrays.","category":"section"},{"location":"#Related-Packages","page":"Introduction","title":"Related Packages","text":"If you need alternative block‑matrix representations or additional functionality, consider:\n\nBlockArrays\nBlockMatrices\nBlockDiagonals\nBlockBandedMatrices","category":"section"},{"location":"symmetric/#SymmetricBlockMatrix","page":"SymmetricBlockMatrix","title":"SymmetricBlockMatrix","text":"A SymmetricBlockMatrix is a matrices that is symmetric and (at least) sparse at the block level.   This can, for example, occur when the matrix originates from a fast multipole method (FMM) or any other algorithm that naturally groups rows and columns into interacting blocks (e.g. near‑field interactions).\n\nnote: Note\nNo sanity check is performed that makes sure that not both blocks of a pair of symmetric blocks are stored.\n\nBelow you will find a compact, step‑by‑step guide that shows how to\n\nBuild a symmetric block‑sparse matrix from two lists of dense blocks and their index maps.  \nMultiply it with vectors (including transposes).  \nConvert it to a standard SparseMatrixCSC for comparison.  \nEnable multi‑threaded construction with OhMyThreads.\n\n","category":"section"},{"location":"symmetric/#Constructing-a-SymmetricBlockMatrix","page":"SymmetricBlockMatrix","title":"Constructing a SymmetricBlockMatrix","text":"using CompScienceMeshes, BEAST, H2Trees\nusing UnicodePlots\nusing BlockSparseMatrices\n\n# make sure that only one of the two symmetric  blocks is stored\nstruct GalerkinSymmetricIsNearFunctor{N}\n    isnear::N\nend\n\nfunction (f::GalerkinSymmetricIsNearFunctor)(tree, nodea, nodeb)\n    if H2Trees.isleaf(tree, nodeb)\n        return f.isnear(tree, nodea, nodeb) && nodea > nodeb\n    else\n        return f.isnear(tree, nodea, nodeb)\n    end\nend\n\nm = meshsphere(1.0, 0.1)\nX = raviartthomas(m)\nsizeS = (numfunctions(X), numfunctions(X))\ntree = TwoNTree(X, 0.2)\n\nselfvalues, colvalues, rowvalues = H2Trees.nearinteractions(\n    tree; extractselfvalues=true, isnear=GalerkinSymmetricIsNearFunctor(H2Trees.isnear)\n)\n\ndiagonals = Matrix{Float64}[]\nfor i in eachindex(selfvalues)\n    push!(diagonals, rand(Float64, length(selfvalues[i]), length(selfvalues[i])))\n    diagonals[end] = (diagonals[end] + transpose(diagonals[end])) / 2  # diagonals are symmetric\nend\n\noffdiagonals = Matrix{Float64}[]\nfor i in eachindex(colvalues)\n    push!(offdiagonals, rand(Float64, length(colvalues[i]), length(rowvalues[i])))\nend\n\nS = SymmetricBlockMatrix(diagonals, selfvalues, offdiagonals, colvalues, rowvalues, sizeS)\n\nS now behaves like a regular matrix: you can query its size, inspect its blocks, etc.","category":"section"},{"location":"symmetric/#Matrix–Vector-Products","page":"SymmetricBlockMatrix","title":"Matrix–Vector Products","text":"using CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\n# make sure that only one of the two symmetric  blocks is stored # hide\nstruct GalerkinSymmetricIsNearFunctor{N} # hide\n    isnear::N # hide\nend # hide\n\nfunction (f::GalerkinSymmetricIsNearFunctor)(tree, nodea, nodeb) # hide\n    if H2Trees.isleaf(tree, nodeb) # hide\n        return f.isnear(tree, nodea, nodeb) && nodea > nodeb # hide\n    else # hide\n        return f.isnear(tree, nodea, nodeb) # hide\n    end # hide\nend # hide\n\nm = meshsphere(1.0, 0.1) # hide\nX = raviartthomas(m) # hide\nsizeS = (numfunctions(X), numfunctions(X)) # hide\ntree = TwoNTree(X, 0.2) # hide\n\nselfvalues, colvalues, rowvalues = H2Trees.nearinteractions( # hide\n    tree; extractselfvalues=true, isnear=GalerkinSymmetricIsNearFunctor(H2Trees.isnear) # hide\n) # hide\n\ndiagonals = Matrix{Float64}[] # hide\nfor i in eachindex(selfvalues) # hide\n    push!(diagonals, rand(Float64, length(selfvalues[i]), length(selfvalues[i]))) # hide\n    diagonals[end] = (diagonals[end] + transpose(diagonals[end])) / 2  # diagonals are symmetric # hide\nend # hide\n\noffdiagonals = Matrix{Float64}[] # hide\nfor i in eachindex(colvalues) # hide\n    push!(offdiagonals, rand(Float64, length(colvalues[i]), length(rowvalues[i]))) # hide\nend # hide\n\nS = SymmetricBlockMatrix(diagonals, selfvalues, offdiagonals, colvalues, rowvalues, sizeS) # hide\n\ny = randn(Float64, numfunctions(X))\n@time S * y\n@time S' * y\n@time transpose(S) * y\n@show maximum(abs, S * y - transpose(S) * y)\nnothing # hide\n\nAll three operations are implemented in pure Julia and respect the block‑sparsity, so they are typically much faster than converting the matrix to a dense format first.","category":"section"},{"location":"symmetric/#Converting-to-a-Classical-Sparse-Matrix","page":"SymmetricBlockMatrix","title":"Converting to a Classical Sparse Matrix","text":"Sometimes you need a SparseMatrixCSC. The conversion is straightforward. You can compare the memory footprints:\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\n# make sure that only one of the two symmetric  blocks is stored # hide\nstruct GalerkinSymmetricIsNearFunctor{N} # hide\n    isnear::N # hide\nend # hide\n\nfunction (f::GalerkinSymmetricIsNearFunctor)(tree, nodea, nodeb) # hide\n    if H2Trees.isleaf(tree, nodeb) # hide\n        return f.isnear(tree, nodea, nodeb) && nodea > nodeb # hide\n    else # hide\n        return f.isnear(tree, nodea, nodeb) # hide\n    end # hide\nend # hide\n\nm = meshsphere(1.0, 0.1) # hide\nX = raviartthomas(m) # hide\nsizeS = (numfunctions(X), numfunctions(X)) # hide\ntree = TwoNTree(X, 0.2) # hide\n\nselfvalues, colvalues, rowvalues = H2Trees.nearinteractions( # hide\n    tree; extractselfvalues=true, isnear=GalerkinSymmetricIsNearFunctor(H2Trees.isnear) # hide\n) # hide\n\ndiagonals = Matrix{Float64}[] # hide\nfor i in eachindex(selfvalues) # hide\n    push!(diagonals, rand(Float64, length(selfvalues[i]), length(selfvalues[i]))) # hide\n    diagonals[end] = (diagonals[end] + transpose(diagonals[end])) / 2  # diagonals are symmetric # hide\nend # hide\n\noffdiagonals = Matrix{Float64}[] # hide\nfor i in eachindex(colvalues) # hide\n    push!(offdiagonals, rand(Float64, length(colvalues[i]), length(rowvalues[i]))) # hide\nend # hide\n\nS = SymmetricBlockMatrix(diagonals, selfvalues, offdiagonals, colvalues, rowvalues, sizeS) # hide\n\nusing SparseArrays\nSsparse = sparse(S)\ndisplay(Ssparse);\n\n@show Base.format_bytes(Base.summarysize(S))\n@show Base.format_bytes(Base.summarysize(Ssparse))\nnothing # hide\n\nIn specific examples SymmetricBlockMatrix consumes less memory.","category":"section"},{"location":"symmetric/#Multi‑Threaded-Construction-(Optional)","page":"SymmetricBlockMatrix","title":"Multi‑Threaded Construction (Optional)","text":"Depending on the example, the block-coloring step can be a bottleneck, thus multithreading is switched-off by default. To enable multithreading you can determine a scheduler from OhMyThreads.\n\nusing CompScienceMeshes, BEAST, H2Trees # hide\nusing UnicodePlots # hide\nusing BlockSparseMatrices # hide\n\n# make sure that only one of the two symmetric  blocks is stored  # hide\nstruct GalerkinSymmetricIsNearFunctor{N} # hide\n    isnear::N # hide\nend # hide\n\nfunction (f::GalerkinSymmetricIsNearFunctor)(tree, nodea, nodeb) # hide\n    if H2Trees.isleaf(tree, nodeb) # hide\n        return f.isnear(tree, nodea, nodeb) && nodea > nodeb # hide\n    else # hide\n        return f.isnear(tree, nodea, nodeb) # hide\n    end # hide\nend # hide\n\nm = meshsphere(1.0, 0.1) # hide\nX = raviartthomas(m) # hide\nsizeS = (numfunctions(X), numfunctions(X)) # hide\ntree = TwoNTree(X, 0.2) # hide\n\nselfvalues, colvalues, rowvalues = H2Trees.nearinteractions(  # hide\n    tree; extractselfvalues=true, isnear=GalerkinSymmetricIsNearFunctor(H2Trees.isnear)  # hide\n) # hide\n\ndiagonals = Matrix{Float64}[] # hide\nfor i in eachindex(selfvalues) # hide\n    push!(diagonals, rand(Float64, length(selfvalues[i]), length(selfvalues[i]))) # hide\n    diagonals[end] = (diagonals[end] + transpose(diagonals[end])) / 2  # diagonals are symmetric  # hide\nend  # hide\n\noffdiagonals = Matrix{Float64}[] # hide\nfor i in eachindex(colvalues) # hide\n    push!(offdiagonals, rand(Float64, length(colvalues[i]), length(rowvalues[i]))) # hide\nend # hide\n\nusing OhMyThreads\nS = SymmetricBlockMatrix(\n    diagonals,\n    selfvalues,\n    offdiagonals,\n    colvalues,\n    rowvalues,\n    sizeS;\n    scheduler=DynamicScheduler(),\n)\nnothing # hide","category":"section"}]
}
